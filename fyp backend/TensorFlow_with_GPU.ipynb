{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "C2iAsgQwI8ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.cm as cm\n",
        "from scipy import ndimage\n",
        "from skimage.measure import regionprops\n",
        "from skimage import io\n",
        "from skimage.filters import threshold_otsu   # For finding the threshold for grayscale to binary conversion\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from time import time\n",
        "import keras\n",
        "from tensorflow.python.framework import ops\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lgGmavP1hdk",
        "outputId": "ad4ed205-a823-47ce-c7fe-8e7d734ebcc5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "0c-4GTtJJCDE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aecaf7c",
        "outputId": "a42384b3-6c93-41ec-c9e8-98017a200bb6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Path Define"
      ],
      "metadata": {
        "id": "5Ous9BLCKs1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genuine_image_paths = \"/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/real\"\n",
        "forged_image_paths = \"/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/forged\""
      ],
      "metadata": {
        "id": "s8tKZcoS2snj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Path Verification"
      ],
      "metadata": {
        "id": "MfW87-0GK2Xc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f54d999",
        "outputId": "a682957e-47b9-470b-f6b3-b1986587412f"
      },
      "source": [
        "import os\n",
        "\n",
        "if os.path.exists(genuine_image_paths):\n",
        "    print(f\"Genuine image path exists: {genuine_image_paths}\")\n",
        "    print(\"Files in genuine images directory:\")\n",
        "    try:\n",
        "        print(os.listdir(genuine_image_paths))\n",
        "    except Exception as e:\n",
        "        print(f\"Could not list files in genuine images directory: {e}\")\n",
        "else:\n",
        "    print(f\"Genuine image path does not exist: {genuine_image_paths}\")\n",
        "\n",
        "if os.path.exists(forged_image_paths):\n",
        "    print(f\"Forged image path exists: {forged_image_paths}\")\n",
        "    print(\"Files in forged images directory:\")\n",
        "    try:\n",
        "        print(os.listdir(forged_image_paths))\n",
        "    except Exception as e:\n",
        "        print(f\"Could not list files in forged images directory: {e}\")\n",
        "else:\n",
        "    print(f\"Forged image path does not exist: {forged_image_paths}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Genuine image path exists: /content/drive/My Drive/FYP/Signature-Forgery-Detection-main/real\n",
            "Files in genuine images directory:\n",
            "['001001_002.png', '002002_004.png', '002002_003.png', '003003_002.png', '004004_000.png', '001001_003.png', '003003_004.png', '003003_001.png', '003003_000.png', '002002_002.png', '002002_001.png', '002002_000.png', '001001_004.png', '001001_001.png', '001001_000.png', '003003_003.png', '004004_003.png', '006006_003.png', '006006_004.png', '008008_000.png', '006006_002.png', '005005_002.png', '005005_000.png', '005005_003.png', '006006_001.png', '005005_001.png', '004004_001.png', '006006_000.png', '004004_002.png', '007007_004.png', '007007_002.png', '007007_001.png', '007007_000.png', '005005_004.png', '007007_003.png', '004004_004.png', '009009_002.png', '008008_004.png', '012012_000.png', '011011_002.png', '008008_003.png', '008008_001.png', '010010_004.png', '009009_004.png', '012012_001.png', '012012_002.png', '011011_004.png', '010010_001.png', '011011_001.png', '010010_003.png', '008008_002.png', '010010_000.png', '010010_002.png', '009009_001.png', '009009_000.png', '011011_003.png', '011011_000.png', '009009_003.png', '012012_004.png', '012012_003.png']\n",
            "Forged image path exists: /content/drive/My Drive/FYP/Signature-Forgery-Detection-main/forged\n",
            "Files in forged images directory:\n",
            "['021002_000.png', '021002_001.png', '021001_000.png', '021001_003.png', '021001_001.png', '021001_002.png', '021001_004.png', '021004_004.png', '021005_000.png', '021004_002.png', '021005_002.png', '021004_000.png', '021004_003.png', '021002_003.png', '021003_001.png', '021002_002.png', '021003_002.png', '021004_001.png', '021002_004.png', '021003_000.png', '021005_001.png', '021003_004.png', '021003_003.png', '021007_002.png', '021006_004.png', '021006_003.png', '021007_000.png', '021005_004.png', '021005_003.png', '021008_001.png', '021007_001.png', '021006_001.png', '021006_002.png', '021008_000.png', '021008_002.png', '021006_000.png', '021008_003.png', '021007_003.png', '021007_004.png', '021012_001.png', '021010_002.png', '021009_002.png', '021010_000.png', '021010_003.png', '021010_001.png', '021008_004.png', '021011_002.png', '021009_004.png', '021009_000.png', '021009_001.png', '021011_004.png', '021012_000.png', '021011_003.png', '021010_004.png', '021011_000.png', '021009_003.png', '021011_001.png', '021012_002.png', '021012_003.png', '021012_004.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the images"
      ],
      "metadata": {
        "id": "4HIWid3kK7d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rgbgrey(img):\n",
        "    # Converts rgb to grayscale\n",
        "    greyimg = np.zeros((img.shape[0], img.shape[1]))\n",
        "    for row in range(len(img)):\n",
        "        for col in range(len(img[row])):\n",
        "            greyimg[row][col] = np.average(img[row][col])\n",
        "    return greyimg"
      ],
      "metadata": {
        "id": "ZQROS-XQ5ah3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greybin(img):\n",
        "    # Converts grayscale to binary\n",
        "    blur_radius = 0.8\n",
        "    img = ndimage.gaussian_filter(img, blur_radius)  # to remove small components or noise\n",
        "#     img = ndimage.binary_erosion(img).astype(img.dtype)\n",
        "    thres = threshold_otsu(img)\n",
        "    binimg = img > thres\n",
        "    binimg = np.logical_not(binimg)\n",
        "    return binimg"
      ],
      "metadata": {
        "id": "yMbpchOO5iG_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preproc(path, img=None, display=True):\n",
        "    if img is None:\n",
        "        img = mpimg.imread(path)\n",
        "    if display:\n",
        "        plt.imshow(img)\n",
        "        plt.show()\n",
        "    grey = rgbgrey(img) #rgb to grey\n",
        "    if display:\n",
        "        plt.imshow(grey, cmap = matplotlib.cm.Greys_r)\n",
        "        plt.show()\n",
        "    binimg = greybin(grey) #grey to binary\n",
        "    if display:\n",
        "        plt.imshow(binimg, cmap = matplotlib.cm.Greys_r)\n",
        "        plt.show()\n",
        "    r, c = np.where(binimg==1)\n",
        "    # Now we will make a bounding box with the boundary as the position of pixels on extreme.\n",
        "    # Thus we will get a cropped image with only the signature part.\n",
        "    signimg = binimg[r.min(): r.max(), c.min(): c.max()]\n",
        "    if display:\n",
        "        plt.imshow(signimg, cmap = matplotlib.cm.Greys_r)\n",
        "        plt.show()\n",
        "    return signimg"
      ],
      "metadata": {
        "id": "1LHoHVwu5jY9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "cd0dFypqLJfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Ratio(img):\n",
        "    a = 0\n",
        "    for row in range(len(img)):\n",
        "        for col in range(len(img[0])):\n",
        "            if img[row][col]==True:\n",
        "                a = a+1\n",
        "    total = img.shape[0] * img.shape[1]\n",
        "    return a/total"
      ],
      "metadata": {
        "id": "QgYRAi5t5nA6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Centroid(img):\n",
        "    numOfWhites = 0\n",
        "    a = np.array([0,0])\n",
        "    for row in range(len(img)):\n",
        "        for col in range(len(img[0])):\n",
        "            if img[row][col]==True:\n",
        "                b = np.array([row,col])\n",
        "                a = np.add(a,b)\n",
        "                numOfWhites += 1\n",
        "    rowcols = np.array([img.shape[0], img.shape[1]])\n",
        "    centroid = a/numOfWhites\n",
        "    centroid = centroid/rowcols\n",
        "    return centroid[0], centroid[1]"
      ],
      "metadata": {
        "id": "-Z8pKuvj5ro2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EccentricitySolidity(img):\n",
        "    r = regionprops(img.astype(\"int8\"))\n",
        "    return r[0].eccentricity, r[0].solidity"
      ],
      "metadata": {
        "id": "8CqmWLWw5uLl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SkewKurtosis(img):\n",
        "    h,w = img.shape\n",
        "    x = range(w)  # cols value\n",
        "    y = range(h)  # rows value\n",
        "    #calculate projections along the x and y axes\n",
        "    xp = np.sum(img,axis=0)\n",
        "    yp = np.sum(img,axis=1)\n",
        "    #centroid\n",
        "    cx = np.sum(x*xp)/np.sum(xp)\n",
        "    cy = np.sum(y*yp)/np.sum(yp)\n",
        "    #standard deviation\n",
        "    x2 = (x-cx)**2\n",
        "    y2 = (y-cy)**2\n",
        "    sx = np.sqrt(np.sum(x2*xp)/np.sum(img))\n",
        "    sy = np.sqrt(np.sum(y2*yp)/np.sum(img))\n",
        "\n",
        "    #skewness\n",
        "    x3 = (x-cx)**3\n",
        "    y3 = (y-cy)**3\n",
        "    skewx = np.sum(xp*x3)/(np.sum(img) * sx**3)\n",
        "    skewy = np.sum(yp*y3)/(np.sum(img) * sy**3)\n",
        "\n",
        "    #Kurtosis\n",
        "    x4 = (x-cx)**4\n",
        "    y4 = (y-cy)**4\n",
        "    # 3 is subtracted to calculate relative to the normal distribution\n",
        "    kurtx = np.sum(xp*x4)/(np.sum(img) * sx**4) - 3\n",
        "    kurty = np.sum(yp*y4)/(np.sum(img) * sy**4) - 3\n",
        "\n",
        "    return (skewx , skewy), (kurtx, kurty)"
      ],
      "metadata": {
        "id": "jsN-7xrg5xy1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getFeatures(path, img=None, display=False):\n",
        "    if img is None:\n",
        "        img = mpimg.imread(path)\n",
        "    img = preproc(path, display=display)\n",
        "    ratio = Ratio(img)\n",
        "    centroid = Centroid(img)\n",
        "    eccentricity, solidity = EccentricitySolidity(img)\n",
        "    skewness, kurtosis = SkewKurtosis(img)\n",
        "    retVal = (ratio, centroid, eccentricity, solidity, skewness, kurtosis)\n",
        "    return retVal"
      ],
      "metadata": {
        "id": "LkuBPj8x5zdR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getCSVFeatures(path, img=None, display=False):\n",
        "    if img is None:\n",
        "        img = mpimg.imread(path)\n",
        "    temp = getFeatures(path, display=display)\n",
        "    features = (temp[0], temp[1][0], temp[1][1], temp[2], temp[3], temp[4][0], temp[4][1], temp[5][0], temp[5][1])\n",
        "    return features"
      ],
      "metadata": {
        "id": "Se75glF253PP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the features"
      ],
      "metadata": {
        "id": "OGFWX_DaLOvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def makeCSV():\n",
        "    if not(os.path.exists('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features')):\n",
        "        os.mkdir('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features')\n",
        "        print('New folder \"Features\" created')\n",
        "    if not(os.path.exists('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Training')):\n",
        "        os.mkdir('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Training')\n",
        "        print('New folder \"Features/Training\" created')\n",
        "    if not(os.path.exists('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Testing')):\n",
        "        os.mkdir('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Testing')\n",
        "        print('New folder \"Features/Testing\" created')\n",
        "    # genuine signatures path\n",
        "    gpath = genuine_image_paths\n",
        "    # forged signatures path\n",
        "    fpath = forged_image_paths\n",
        "    for person in range(1,13):\n",
        "        per = ('00'+str(person))[-3:]\n",
        "        print('Saving features for person id-',per)\n",
        "\n",
        "        with open('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Training/training_'+per+'.csv', 'w') as handle:\n",
        "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
        "            # Training set\n",
        "            for i in range(0,3):\n",
        "                source = os.path.join(gpath, per+per+'_00'+str(i)+'.png')\n",
        "                features = getCSVFeatures(path=source)\n",
        "                handle.write(','.join(map(str, features))+',1\\n')\n",
        "            for i in range(0,3):\n",
        "                source = os.path.join(fpath, '021'+per+'_00'+str(i)+'.png')\n",
        "                features = getCSVFeatures(path=source)\n",
        "                handle.write(','.join(map(str, features))+',0\\n')\n",
        "\n",
        "        with open('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Testing/testing_'+per+'.csv', 'w') as handle:\n",
        "            handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y,output\\n')\n",
        "            # Testing set\n",
        "            for i in range(3, 5):\n",
        "                source = os.path.join(gpath, per+per+'_00'+str(i)+'.png')\n",
        "                features = getCSVFeatures(path=source)\n",
        "                handle.write(','.join(map(str, features))+',1\\n')\n",
        "            for i in range(3,5):\n",
        "                source = os.path.join(fpath, '021'+per+'_00'+str(i)+'.png')\n",
        "                features = getCSVFeatures(path=source)\n",
        "                handle.write(','.join(map(str, features))+',0\\n')"
      ],
      "metadata": {
        "id": "tUEuAqVw6AP_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "makeCSV()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAolVLeH6hUw",
        "outputId": "04c43c4f-7533-4565-e97e-e24beafc5eef"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving features for person id- 001\n",
            "Saving features for person id- 002\n",
            "Saving features for person id- 003\n",
            "Saving features for person id- 004\n",
            "Saving features for person id- 005\n",
            "Saving features for person id- 006\n",
            "Saving features for person id- 007\n",
            "Saving features for person id- 008\n",
            "Saving features for person id- 009\n",
            "Saving features for person id- 010\n",
            "Saving features for person id- 011\n",
            "Saving features for person id- 012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF Model"
      ],
      "metadata": {
        "id": "bRJCpdQsLWIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def testing(path):\n",
        "    feature = getCSVFeatures(path)\n",
        "    if not(os.path.exists('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/TestFeatures')):\n",
        "        os.mkdir('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/TestFeatures')\n",
        "    with open('/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/TestFeatures/testcsv.csv', 'w') as handle:\n",
        "        handle.write('ratio,cent_y,cent_x,eccentricity,solidity,skew_x,skew_y,kurt_x,kurt_y\\n')\n",
        "        handle.write(','.join(map(str, feature))+'\\n')"
      ],
      "metadata": {
        "id": "nZbttmXt9_4U"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_input = 9\n",
        "train_person_id = input(\"Enter person's id : \")\n",
        "test_image_path = input(\"Enter path of signature image : \")\n",
        "train_path = '/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Training/training_'+train_person_id+'.csv'\n",
        "testing(test_image_path)\n",
        "test_path = '/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/TestFeatures/testcsv.csv'\n",
        "\n",
        "def readCSV(train_path, test_path, type2=False):\n",
        "    # Reading train data\n",
        "    df = pd.read_csv(train_path, usecols=range(n_input))\n",
        "    train_input = np.array(df.values)\n",
        "    train_input = train_input.astype(np.float32, copy=False)  # Converting input to float_32\n",
        "    df = pd.read_csv(train_path, usecols=(n_input,))\n",
        "    temp = [elem[0] for elem in df.values]\n",
        "    correct = np.array(temp)\n",
        "    corr_train = keras.utils.to_categorical(correct,2)      # Converting to one hot\n",
        "    # Reading test data\n",
        "    df = pd.read_csv(test_path, usecols=range(n_input))\n",
        "    test_input = np.array(df.values)\n",
        "    test_input = test_input.astype(np.float32, copy=False)\n",
        "    if not(type2):\n",
        "        df = pd.read_csv(test_path, usecols=(n_input,))\n",
        "        temp = [elem[0] for elem in df.values]\n",
        "        correct = np.array(temp)\n",
        "        corr_test = kearas.utils.to_categorical(correct,2)      # Converting to one hot\n",
        "    if not(type2):\n",
        "        return train_input, corr_train, test_input, corr_test\n",
        "    else:\n",
        "        return train_input, corr_train, test_input\n",
        "\n",
        "ops.reset_default_graph()\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 1000\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 7 # 1st layer number of neurons\n",
        "n_hidden_2 = 10 # 2nd layer number of neurons\n",
        "n_hidden_3 = 30 # 3rd layer\n",
        "n_classes = 2 # no. of classes (genuine or forged)\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], seed=1)),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], seed=2))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1], seed=3)),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes], seed=4))\n",
        "}\n",
        "\n",
        "\n",
        "# Create model\n",
        "def multilayer_perceptron(x):\n",
        "    layer_1 = tf.tanh((tf.matmul(x, weights['h1']) + biases['b1']))\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    out_layer = tf.tanh(tf.matmul(layer_1, weights['out']) + biases['out'])\n",
        "    return out_layer\n",
        "\n",
        "# Construct model\n",
        "logits = multilayer_perceptron(X)\n",
        "\n",
        "# Define loss and optimizer\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.squared_difference(logits, Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# For accuracies\n",
        "pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "def evaluate(train_path, test_path, type2=False):\n",
        "    if not(type2):\n",
        "        train_input, corr_train, test_input, corr_test = readCSV(train_path, test_path)\n",
        "    else:\n",
        "        train_input, corr_train, test_input = readCSV(train_path, test_path, type2)\n",
        "    ans = 'Random'\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        # Training cycle\n",
        "        for epoch in range(training_epochs):\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, cost = sess.run([train_op, loss_op], feed_dict={X: train_input, Y: corr_train})\n",
        "            if cost<0.0001:\n",
        "                break\n",
        "#             # Display logs per epoch step\n",
        "#             if epoch % 999 == 0:\n",
        "#                 print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(cost))\n",
        "#         print(\"Optimization Finished!\")\n",
        "\n",
        "        # Finding accuracies\n",
        "        accuracy1 =  accuracy.eval({X: train_input, Y: corr_train})\n",
        "#         print(\"Accuracy for train:\", accuracy1)\n",
        "#         print(\"Accuracy for test:\", accuracy2)\n",
        "        if type2 is False:\n",
        "            accuracy2 =  accuracy.eval({X: test_input, Y: corr_test})\n",
        "            return accuracy1, accuracy2\n",
        "        else:\n",
        "            prediction = pred.eval({X: test_input})\n",
        "            if prediction[0][1]>prediction[0][0]:\n",
        "                print('Genuine Image')\n",
        "                return True\n",
        "            else:\n",
        "                print('Forged Image')\n",
        "                return False\n",
        "\n",
        "\n",
        "def trainAndTest(rate=0.001, epochs=1700, neurons=7, display=False):\n",
        "    start = time()\n",
        "\n",
        "    # Parameters\n",
        "    global training_rate, training_epochs, n_hidden_1\n",
        "    learning_rate = rate\n",
        "    training_epochs = epochs\n",
        "\n",
        "    # Network Parameters\n",
        "    n_hidden_1 = neurons # 1st layer number of neurons\n",
        "    n_hidden_2 = 7 # 2nd layer number of neurons\n",
        "    n_hidden_3 = 30 # 3rd layer\n",
        "\n",
        "    train_avg, test_avg = 0, 0\n",
        "    n = 10\n",
        "    for i in range(1,n+1):\n",
        "        if display:\n",
        "            print(\"Running for Person id\",i)\n",
        "        temp = ('0'+str(i))[-2:]\n",
        "        train_score, test_score = evaluate(train_path.replace('01',temp), test_path.replace('01',temp))\n",
        "        train_avg += train_score\n",
        "        test_avg += test_score\n",
        "    if display:\n",
        "#         print(\"Number of neurons in Hidden layer-\", n_hidden_1)\n",
        "        print(\"Training average-\", train_avg/n)\n",
        "        print(\"Testing average-\", test_avg/n)\n",
        "        print(\"Time taken-\", time()-start)\n",
        "    return train_avg/n, test_avg/n, (time()-start)/n\n",
        "\n",
        "\n",
        "evaluate(train_path, test_path, type2=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "BYbcvTbhFl_B",
        "outputId": "89365e05-c9fe-4714-d982-61aff1fdb37c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter person's id : 009\n",
            "Enter path of signature image : /content/drive/My Drive/FYP/Signature-Forgery-Detection-main/real/001001_000.png\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Operation' object has no attribute 'save'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-4175654875.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mtrain_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/my_model.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;31m# For accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply softmax to logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Operation' object has no attribute 'save'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_input = 9\n",
        "train_person_id = input(\"Enter person's id : \")\n",
        "test_image_path = input(\"Enter path of signature image : \")\n",
        "train_path = '/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Training/training_'+train_person_id+'.csv'\n",
        "testing(test_image_path)\n",
        "# test_path is defined within trainAndTest for dataset evaluation\n",
        "\n",
        "def readCSV(train_path, test_path, type2=False):\n",
        "    # Reading train data\n",
        "    df = pd.read_csv(train_path, usecols=range(n_input))\n",
        "    train_input = np.array(df.values)\n",
        "    train_input = train_input.astype(np.float32, copy=False)  # Converting input to float_32\n",
        "    df = pd.read_csv(train_path, usecols=(n_input,))\n",
        "    temp = [elem[0] for elem in df.values]\n",
        "    correct = np.array(temp)\n",
        "    corr_train = keras.utils.to_categorical(correct,2)      # Converting to one hot\n",
        "    # Reading test data\n",
        "    df = pd.read_csv(test_path, usecols=range(n_input))\n",
        "    test_input = np.array(df.values)\n",
        "    test_input = test_input.astype(np.float32, copy=False)\n",
        "    if not(type2): # Only read correct test labels if type2 is False and it's not the single test image file\n",
        "        try:\n",
        "            df = pd.read_csv(test_path, usecols=(n_input,))\n",
        "            temp = [elem[0] for elem in df.values]\n",
        "            correct = np.array(temp)\n",
        "            corr_test = keras.utils.to_categorical(correct,2)      # Converting to one hot\n",
        "            return train_input, corr_train, test_input, corr_test\n",
        "        except ValueError:\n",
        "            # If the test file doesn't have the output column, return only the inputs\n",
        "            return train_input, corr_train, test_input, None\n",
        "    else:\n",
        "        return train_input, corr_train, test_input\n",
        "\n",
        "ops.reset_default_graph()\n",
        "# Parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 1000\n",
        "display_step = 1\n",
        "\n",
        "# Network Parameters\n",
        "n_hidden_1 = 7 # 1st layer number of neurons\n",
        "n_hidden_2 = 10 # 2nd layer number of neurons\n",
        "n_hidden_3 = 30 # 3rd layer\n",
        "n_classes = 2 # no. of classes (genuine or forged)\n",
        "\n",
        "# tf Graph input\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# Store layers weight & bias\n",
        "weights = {\n",
        "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], seed=1)),\n",
        "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes], seed=2))\n",
        "}\n",
        "biases = {\n",
        "    'b1': tf.Variable(tf.random_normal([n_hidden_1], seed=3)),\n",
        "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "    'out': tf.Variable(tf.random_normal([n_classes], seed=4))\n",
        "}\n",
        "\n",
        "\n",
        "# Create model\n",
        "def multilayer_perceptron(x):\n",
        "    layer_1 = tf.tanh((tf.matmul(x, weights['h1']) + biases['b1']))\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
        "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
        "    out_layer = tf.tanh(tf.matmul(layer_1, weights['out']) + biases['out'])\n",
        "    return out_layer\n",
        "\n",
        "# Construct model\n",
        "logits = multilayer_perceptron(X)\n",
        "\n",
        "# Define loss and optimizer\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.squared_difference(logits, Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# For accuracies\n",
        "pred = tf.nn.softmax(logits)  # Apply softmax to logits\n",
        "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "# Initializing the variables\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "def evaluate(train_path, test_path, type2=False):\n",
        "    if not(type2):\n",
        "        train_input, corr_train, test_input, corr_test = readCSV(train_path, test_path)\n",
        "    else:\n",
        "        train_input, corr_train, test_input = readCSV(train_path, test_path, type2)\n",
        "    ans = 'Random'\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "        # Training cycle\n",
        "        for epoch in range(training_epochs):\n",
        "            # Run optimization op (backprop) and cost op (to get loss value)\n",
        "            _, cost = sess.run([train_op, loss_op], feed_dict={X: train_input, Y: corr_train})\n",
        "            if cost<0.0001:\n",
        "                break\n",
        "#             # Display logs per epoch step\n",
        "#             if epoch % 999 == 0:\n",
        "#                 print(\"Epoch:\", '%04d' % (epoch+1), \"cost={:.9f}\".format(cost))\n",
        "#         print(\"Optimization Finished!\")\n",
        "\n",
        "        # Finding accuracies\n",
        "        accuracy1 =  accuracy.eval({X: train_input, Y: corr_train})\n",
        "        if type2 is False:\n",
        "            if corr_test is not None:\n",
        "                accuracy2 =  accuracy.eval({X: test_input, Y: corr_test})\n",
        "                print(\"Accuracy for train:\", accuracy1) # Corrected indentation\n",
        "                print(\"Accuracy for test:\", accuracy2)   # Corrected indentation\n",
        "                return accuracy1, accuracy2\n",
        "            else:\n",
        "                # Cannot calculate test accuracy without correct labels\n",
        "                print(\"Accuracy for train:\", accuracy1) # Corrected indentation\n",
        "                print(\"Cannot calculate test accuracy: Test file does not contain output column.\")\n",
        "                return accuracy1, None\n",
        "        else:\n",
        "            prediction = pred.eval({X: test_input})\n",
        "            # Print confidence scores as percentages\n",
        "            print(f\"Confidence (Forged): {prediction[0][0]*100:.2f}%\")\n",
        "            print(f\"Confidence (Genuine): {prediction[0][1]*100:.2f}%\")\n",
        "            if prediction[0][1]>prediction[0][0]:\n",
        "                print('Genuine Image')\n",
        "                return True\n",
        "            else:\n",
        "                print('Forged Image')\n",
        "                return False\n",
        "\n",
        "\n",
        "# evaluate(train_path, test_path, type2=True) # Commented out the original call for single image\n",
        "\n",
        "\n",
        "def trainAndTest(rate=0.001, epochs=1700, neurons=7, display=False):\n",
        "    start = time()\n",
        "\n",
        "    # Parameters\n",
        "    global learning_rate, training_epochs, n_hidden_1\n",
        "    learning_rate = rate\n",
        "    training_epochs = epochs\n",
        "\n",
        "    # Network Parameters\n",
        "    n_hidden_1 = neurons # 1st layer number of neurons\n",
        "    n_hidden_2 = 7 # 2nd layer number of neurons\n",
        "    n_hidden_3 = 30 # 3rd layer\n",
        "\n",
        "    train_avg, test_avg = 0, 0\n",
        "    n = 10 # Number of persons to test on\n",
        "    valid_persons_for_test_avg = 0 # Counter for persons with valid test data\n",
        "\n",
        "    for i in range(1,n+1):\n",
        "        if display:\n",
        "            print(\"Running for Person id\",i)\n",
        "        temp = ('0'+str(i))[-2:]\n",
        "        # Construct the correct paths for the training and testing dataset files\n",
        "        current_train_path = '/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Training/training_'+temp+'.csv'\n",
        "        current_test_path = '/content/drive/My Drive/FYP/Signature-Forgery-Detection-main/Features/Testing/testing_'+temp+'.csv'\n",
        "\n",
        "        # Call evaluate with type2=False for dataset evaluation\n",
        "        train_score, test_score = evaluate(current_train_path, current_test_path, type2=False)\n",
        "\n",
        "        # Accumulate training accuracy\n",
        "        train_avg += train_score\n",
        "\n",
        "        # Accumulate test accuracy only if valid\n",
        "        if test_score is not None:\n",
        "            test_avg += test_score\n",
        "            valid_persons_for_test_avg += 1 # Increment counter\n",
        "\n",
        "    # Avoid division by zero if no valid test data was processed\n",
        "    if valid_persons_for_test_avg > 0:\n",
        "        if display:\n",
        "    #         print(\"Number of neurons in Hidden layer-\", n_hidden_1)\n",
        "            print(\"Training average-\", train_avg/n) # Training average is still over all persons attempted\n",
        "            print(\"Testing average-\", test_avg/valid_persons_for_test_avg) # Test average is over valid persons\n",
        "            print(\"Time taken-\", time()-start)\n",
        "        return train_avg/n, test_avg/valid_persons_for_test_avg, (time()-start)/n\n",
        "    else:\n",
        "        print(\"No valid test data found to calculate average accuracy.\")\n",
        "        # Return None for accuracies and time if no valid test data\n",
        "        return None, None, None\n",
        "\n",
        "# Original call to evaluate a single image\n",
        "evaluate(train_path, test_path, type2=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UublNiqhKh2H",
        "outputId": "6c65be59-348c-4766-dc2f-6254fe701333"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter person's id : 009\n",
            "Enter path of signature image : /content/drive/My Drive/FYP/Signature-Forgery-Detection-main/real/001001_000.png\n",
            "Confidence (Forged): 82.31%\n",
            "Confidence (Genuine): 17.69%\n",
            "Forged Image\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/my_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "UoLbmYhKcFLH",
        "outputId": "1fbb396d-8745-4bd3-f3ea-3e722fb695f1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-3381406020.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/my_model.keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}